[
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "To start, we will make a file named perceptron.py and import Perceptron and Percptron Optimizer. The first 2 lines of code automatically load any changes made to the perceptron.py file. The link to the perceptron.py file is given below. https://github.com/RobsanD23/github_RobsanD23/blob/main/posts/perceptron/perceptron.py\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport torch\nimport matplotlib.pyplot as plt\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n#Define X\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0:\n    loss = p.loss(X,y)\n    loss_vec.append(loss)\n\n    i = torch.randint(n,size= (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    opt.step(x_i, y_i)\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[17], line 18\n     15 x_i = X[[i],:]\n     16 y_i = y[i]\n---&gt; 18 opt.step(x_i, y_i)\n\nFile c:\\Users\\robsa\\OneDrive\\Documents\\machine_learning\\github_RobsanD23\\posts\\perceptron\\perceptron.py:93, in PerceptronOptimizer.step(self, X, y)\n     87 \"\"\"\n     88 Compute one step of the perceptron update using the feature matrix X \n     89 and target vector y. \n     90 \"\"\"\n     91 self.model.loss(X, y)\n---&gt; 93 self.model.w = self.model.w - self.model.grad(X,y)\n\nFile c:\\Users\\robsa\\OneDrive\\Documents\\machine_learning\\github_RobsanD23\\posts\\perceptron\\perceptron.py:77, in Perceptron.grad(self, X, y)\n     75 def grad(self, X, y):\n     76     s = self.score(X)\n---&gt; 77     sign = 1 if s*(2*y - 1) &lt; 0 else 0\n     78     return sign * (2*y-1)* X\n\nKeyboardInterrupt: \n\n\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/logisitc_regression/index.html",
    "href": "posts/logisitc_regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LinearModel, LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/logisitc_regression/index.html#vanilla-gradient-descent",
    "href": "posts/logisitc_regression/index.html#vanilla-gradient-descent",
    "title": "Implementing Logistic Regression",
    "section": "Vanilla Gradient Descent",
    "text": "Vanilla Gradient Descent\nIn this section, we are going to set beta to 0 so that we can visualize “regular” gradient descent (i.e., without momentum). Below we will perform a step for 3000 iterations and at each iteration we with compute the loss and store it in a list.\n\nLR2 = LogisticRegression()\nLR2.score(X)\nopt2 = GradientDescentOptimizer(LR2)\nloss_vec = []\nfor _ in range(3000):\n    loss =LR2.loss(X, y)\n    loss_vec.append(loss) # keep track of the loss over time. \n    opt2.step(X, y, alpha = 0.1, beta = 0)\n    \n\nNow its time to plot the loss after each step of the gradient descent optimizer. As you can see from the graph below, the loss is decreasing at each step. This is a good sign that our gradient descent optimizer is working correctly.\n\nimport matplotlib.pyplot as plt\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\n\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss\", title = \"Regular Gradient Descent\")"
  },
  {
    "objectID": "posts/logisitc_regression/index.html#benefits-of-momentum",
    "href": "posts/logisitc_regression/index.html#benefits-of-momentum",
    "title": "Implementing Logistic Regression",
    "section": "Benefits of Momentum",
    "text": "Benefits of Momentum\nNow we are going to set beta to 0.9 so that we can take advantage of our implementation of gradient descent with momentum algorithm. To see the effect of momentum we will keep every other argument the same as before and compare the results.\n\nLR3 = LogisticRegression()\nLR3.score(X)\nopt3 = GradientDescentOptimizer(LR3)\nloss_v = []\nfor _ in range(3000):\n    loss =LR3.loss(X, y)\n    loss_v.append(loss) # keep track of the loss over time. \n    opt3.step(X, y, alpha = 0.1, beta = 0.9)\n\nplt.plot(torch.arange(1, len(loss_v)+1), loss_v, color = \"black\")\n\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss\",title = \"Loss with momentum\")\n\n\n\n\n\n\n\n\nAs you can see from the 2 graphs above, gradient descent with momentum is able to achieve optimal loss with fewer iterations than regular gradient descent. For regular gradient descent, it took about 3000 iterations to reach optimal loss, while gradient descent with momentum was able to do it in abut 250 iterations. This is a significant improvement and shows the power of using momentum in gradient descent optimization."
  },
  {
    "objectID": "posts/logisitc_regression/index.html#overfitting",
    "href": "posts/logisitc_regression/index.html#overfitting",
    "title": "Implementing Logistic Regression",
    "section": "Overfitting",
    "text": "Overfitting\nNow we are going to show the effect of overfitting our model on training data. To start we are going to create a training set and a test set shown in the cell below. Note that the number of features is larger than the number of data points.\n\nX_train, y_train = classification_data(50, 0.8, 100)\nX_test, y_test = classification_data(50, 0.8, 100)\n\nNow we are going to train our model on the training set until it achieves 100% accuracy.\n\nLR4 = LogisticRegression()\nLR4.score(X_train)\nopt4 = GradientDescentOptimizer(LR4)\nfor _ in range(100):\n    opt4.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n\nIn the cell below, we calculate the accuracy of our model on the training set and see that it is 100%. In the following cell, we will calculate the accuracy of our model on the test set and see that it is only 78%. This is a significant drop in accuracy and shows that our model is overfitting the training data. This is becuase our model is overfitted on the training data and is not able to generalize to the test data.\n\n(LR4.predict(X_train) == y_train).float().mean()\n\ntensor(1.)\n\n\n\n(LR4.predict(X_test) == y_test ).float().mean()\n\ntensor(0.8400)"
  },
  {
    "objectID": "posts/logisitc_regression/index.html#performance-on-empirical-data",
    "href": "posts/logisitc_regression/index.html#performance-on-empirical-data",
    "title": "Implementing Logistic Regression",
    "section": "Performance on Empirical Data",
    "text": "Performance on Empirical Data\nIn this section, we are going to train and test our model on a real dataset. The imported dataset contains various features and based on these features we are going to decide whether a person dropped out of school of if they graduated. Below we import the dataset and change the data in to torch tensors.\n\nfrom ucimlrepo import fetch_ucirepo \n  \n# fetch dataset \npredict_students_dropout_and_academic_success = fetch_ucirepo(id=697) \n  \n# data (as pandas dataframes) \nX = predict_students_dropout_and_academic_success.data.features\nX = torch.tensor(X.values, dtype = torch.float32) \ny = predict_students_dropout_and_academic_success.data.targets \ny['num'] = (y['Target'] == \"Graduate\").astype(float)\ny = torch.tensor(y['num'], dtype=torch.float32)\n\n  \n# metadata \n# print(predict_students_dropout_and_academic_success.metadata) \n  \n# variable information \n# print(predict_students_dropout_and_academic_success.variables)\n\nC:\\Users\\robsa\\AppData\\Local\\Temp\\ipykernel_48004\\847060615.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  y['num'] = (y['Target'] == \"Graduate\").astype(float)\n\n\nHere we split the data in to a training set and a test set. We will use the training set to train our model and the test set to evaluate the performance of our model.\n\nfrom sklearn.model_selection import train_test_split\n\n# Custom split ratios\ntrain_ratio = 0.6\nvalidation_ratio = 0.2\ntest_ratio = 0.2\n\n# First split: train and temp (validation + test)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - train_ratio), random_state=42)\n\n# Second split: validation and test\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(test_ratio / (test_ratio + validation_ratio)), random_state=42)\n\nNow we will train our model on the training set and see what the accuracy is on it.\n\nLR5 = LogisticRegression()\nLR5.score(X_train)\nopt5 = GradientDescentOptimizer(LR5)\nfor _ in range(100000): \n    opt5.step(X_train, y_train, alpha = 0.1, beta = 0.99)\n(LR5.predict(X_train) == y_train).float().mean()\n\ntensor(0.8255)\n\n\nNow on the test set we see that the accuracy is about 82%.\n\n(LR5.predict(X_test) == y_test).float().mean()\n\ntensor(0.8237)"
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "In this blog, we are going to use ACS data to train a model to predict whether a person is emloyed or not give a list of features. Then, we will analyze the model to see if there is any racial bias in the employment prediction. Lastly, we will discuss the possible effects of this model if it was used in the real world.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"PA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000007\n2\n1\n200\n1\n42\n1013097\n23\n59\n...\n2\n2\n23\n3\n45\n23\n23\n22\n42\n43\n\n\n1\nP\n2018GQ0000009\n2\n1\n3104\n1\n42\n1013097\n72\n39\n...\n72\n15\n73\n132\n18\n18\n126\n71\n73\n15\n\n\n2\nP\n2018GQ0000010\n2\n1\n803\n1\n42\n1013097\n31\n18\n...\n31\n30\n32\n30\n31\n4\n32\n59\n5\n59\n\n\n3\nP\n2018GQ0000032\n2\n1\n3501\n1\n42\n1013097\n92\n20\n...\n93\n14\n13\n16\n89\n95\n90\n90\n93\n13\n\n\n4\nP\n2018GQ0000062\n2\n1\n3302\n1\n42\n1013097\n23\n20\n...\n43\n4\n22\n22\n23\n3\n22\n41\n22\n3\n\n\n\n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n59\n16.0\n1\n16\n1\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n1\n39\n16.0\n5\n16\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n2\n18\n16.0\n5\n17\n2\nNaN\n2\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n3\n20\n19.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n4\n20\n19.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n2\n9\n1.0\n\n\n\n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())]) StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegression?Documentation for LogisticRegressionLogisticRegression() \n\n\n\ny_hat = model.predict(X_test)\n\n\n(y_hat == y_test).mean()\n\n0.7860075927791121\n\n\n\n(y_hat == y_test)[group_test == 1].mean()\n\n0.787089618456078\n\n\n\n(y_hat == y_test)[group_test == 2].mean()\n\n0.7692307692307693\n\n\n\nconfusion_matrix(y_test, y_hat)\n\narray([[10439,  3251],\n       [ 2273,  9851]], dtype=int64)\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel2 = DecisionTreeClassifier(max_depth=5)\nmodel2.fit(X_train, y_train)\n\nDecisionTreeClassifier(max_depth=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=5) \n\n\n\ny2_hat = model2.predict(X_test)\n\n\n(y2_hat == y_test).mean()\n\n0.8241651816843574\n\n\n\n(y2_hat == y_test)[group_test == 1].mean()\n\n0.8258207630878438\n\n\n\n(y2_hat == y_test)[group_test == 2].mean()\n\n0.8094951923076923\n\n\n\nconfusion_matrix(y_test, y2_hat)\n\narray([[11089,  2601],\n       [ 1938, 10186]], dtype=int64)"
  },
  {
    "objectID": "posts/auditing-bias/index.html#abstract",
    "href": "posts/auditing-bias/index.html#abstract",
    "title": "Auditing Bias",
    "section": "",
    "text": "In this blog, we are going to use ACS data to train a model to predict whether a person is emloyed or not give a list of features. Then, we will analyze the model to see if there is any racial bias in the employment prediction. Lastly, we will discuss the possible effects of this model if it was used in the real world.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"PA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000007\n2\n1\n200\n1\n42\n1013097\n23\n59\n...\n2\n2\n23\n3\n45\n23\n23\n22\n42\n43\n\n\n1\nP\n2018GQ0000009\n2\n1\n3104\n1\n42\n1013097\n72\n39\n...\n72\n15\n73\n132\n18\n18\n126\n71\n73\n15\n\n\n2\nP\n2018GQ0000010\n2\n1\n803\n1\n42\n1013097\n31\n18\n...\n31\n30\n32\n30\n31\n4\n32\n59\n5\n59\n\n\n3\nP\n2018GQ0000032\n2\n1\n3501\n1\n42\n1013097\n92\n20\n...\n93\n14\n13\n16\n89\n95\n90\n90\n93\n13\n\n\n4\nP\n2018GQ0000062\n2\n1\n3302\n1\n42\n1013097\n23\n20\n...\n43\n4\n22\n22\n23\n3\n22\n41\n22\n3\n\n\n\n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n59\n16.0\n1\n16\n1\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n1\n39\n16.0\n5\n16\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n2\n18\n16.0\n5\n17\n2\nNaN\n2\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n3\n20\n19.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n4\n20\n19.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n2\n9\n1.0\n\n\n\n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())]) StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegression?Documentation for LogisticRegressionLogisticRegression() \n\n\n\ny_hat = model.predict(X_test)\n\n\n(y_hat == y_test).mean()\n\n0.7860075927791121\n\n\n\n(y_hat == y_test)[group_test == 1].mean()\n\n0.787089618456078\n\n\n\n(y_hat == y_test)[group_test == 2].mean()\n\n0.7692307692307693\n\n\n\nconfusion_matrix(y_test, y_hat)\n\narray([[10439,  3251],\n       [ 2273,  9851]], dtype=int64)\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel2 = DecisionTreeClassifier(max_depth=5)\nmodel2.fit(X_train, y_train)\n\nDecisionTreeClassifier(max_depth=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=5) \n\n\n\ny2_hat = model2.predict(X_test)\n\n\n(y2_hat == y_test).mean()\n\n0.8241651816843574\n\n\n\n(y2_hat == y_test)[group_test == 1].mean()\n\n0.8258207630878438\n\n\n\n(y2_hat == y_test)[group_test == 2].mean()\n\n0.8094951923076923\n\n\n\nconfusion_matrix(y_test, y2_hat)\n\narray([[11089,  2601],\n       [ 1938, 10186]], dtype=int64)"
  },
  {
    "objectID": "posts/auditing-bias/index.html#overall-measures",
    "href": "posts/auditing-bias/index.html#overall-measures",
    "title": "Auditing Bias",
    "section": "Overall Measures",
    "text": "Overall Measures\nWhat is the overall accuracy of your model? : My best performing model was a Decision Tree Classifier with a max depth of 5. The following cells will calculate the accuracy of this model on test data.\n\ncm = confusion_matrix(y_test, y2_hat)\nacc = (cm[0,0] + cm[1,1]) / cm.sum()\naccuracy = f\"The overall accuracy of the model is {acc:.2%}\"\naccuracy\n\n'The overall accuracy of the model is 82.42%'\n\n\n\nppv = cm[1,1] / (cm[1,1] + cm[0,1])\npp_value = f\"The positive predictive value of the model is {ppv:.2%}\"\npp_value\n\n'The positive predictive value of the model is 79.66%'\n\n\n\nfnr = cm[1,0] / (cm[1,0] + cm[1,1])\nfn_rate = f\"The false negative rate of the model is {fnr:.2%}\"\nfn_rate\n\n'The false negative rate of the model is 15.98%'\n\n\n\nfpr = cm[0,1] / (cm[0,1] + cm[0,0])\nfp_rate = f\"The false positive rate of the model is {fpr:.2%}\"\nfp_rate\n\n'The false positive rate of the model is 19.00%'"
  },
  {
    "objectID": "posts/auditing-bias/index.html#group-measures",
    "href": "posts/auditing-bias/index.html#group-measures",
    "title": "Auditing Bias",
    "section": "Group Measures",
    "text": "Group Measures\nWhat is the accuracy of your model for each group? : The following cells will calculate the accuracy of this model on test data for each group.\n\nwcm = confusion_matrix(y_test[group_test == 1], y2_hat[group_test == 1])\nwacc = (wcm[0,0] + wcm[1,1]) / wcm.sum()\nwaccuracy = f\"The accuracy of the model for white people is {wacc:.2%}\"\nwaccuracy\n\n'The accuracy of the model for white people is 82.58%'\n\n\n\nbcm = confusion_matrix(y_test[group_test == 2], y2_hat[group_test == 2])\nbacc = (bcm[0,0] + bcm[1,1]) / bcm.sum()\nbaccuracy = f\"The accuracy of the model for black people is {bacc:.2%}\"\nbaccuracy\n\n'The accuracy of the model for black people is 80.95%'\n\n\n\nocm = confusion_matrix(y_test[group_test &gt; 2], y2_hat[group_test &gt; 2])\noacc = (ocm[0,0] + ocm[1,1]) / ocm.sum()\noaccuracy = f\"The accuracy of the model for other people is {oacc:.2%}\"\noaccuracy\n\n'The accuracy of the model for other people is 81.61%'\n\n\nIn the upcoming cells we will be calculating the positive predictive value (PPV) for each group.\n\nwppv = wcm[1,1] / (wcm[1,1] + wcm[0,1])\nwpp_value = f\"The positive predictive value of the model for white people is {wppv:.2%}\"\nwpp_value\n\n'The positive predictive value of the model for white people is 80.43%'\n\n\n\nbppv = bcm[1,1] / (bcm[1,1] + bcm[0,1])\nbpp_value = f\"The positive predictive value of the model for black people is {bppv:.2%}\"\nbpp_value\n\n'The positive predictive value of the model for black people is 72.16%'\n\n\n\noppv = ocm[1,1] / (ocm[1,1] + ocm[0,1])\nopp_value = f\"The positive predictive value of the model for other people is {oppv:.2%}\"\nopp_value\n\n'The positive predictive value of the model for other people is 74.57%'\n\n\nNext, we will calculate the false positive rate (FPR) and false negative rate (FNR) for each group.\n\nwfnr = wcm[1,0] / (wcm[1,0] + wcm[1,1])\nwfpr = wcm[0,1] / (wcm[0,1] + wcm[0,0])\nwf_rates = f\"The false negative rate of the model for white people is {wfnr:.2%} and the false positive rate is {wfpr:.2%}\"\nwf_rates\n\n'The false negative rate of the model for white people is 15.67% and the false positive rate is 19.04%'\n\n\n\nbfnr = bcm[1,0] / (bcm[1,0] + bcm[1,1])\nbfpr = bcm[0,1] / (bcm[0,1] + bcm[0,0])\nbf_rates = f\"The false negative rate of the model for black people is {bfnr:.2%} and the false positive rate is {bfpr:.2%}\"\nbf_rates\n\n'The false negative rate of the model for black people is 21.80% and the false positive rate is 17.46%'\n\n\n\nofnr = ocm[1,0] / (ocm[1,0] + ocm[1,1])\nofpr = ocm[0,1] / (ocm[0,1] + ocm[0,0])\nof_rates = f\"The false negative rate of the model for other people is {ofnr:.2%} and the false positive rate is {ofpr:.2%}\"\nof_rates\n\n'The false negative rate of the model for other people is 15.79% and the false positive rate is 20.21%'"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Robsan Dinka\n\nThis is my blog. Enjoy!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Auditing Bias\n\n\n\n\n\nCreating a machine learning model that predicts employment status and checking for bias.\n\n\n\n\n\nMar 5, 2025\n\n\nRobsan Dinka\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nBuiliding a logistic regression model from scratch\n\n\n\n\n\nJan 24, 2025\n\n\nRobsan Dinka\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nCreating a machine learning model to classify penguins given a list of attributes and measurements\n\n\n\n\n\nJan 24, 2025\n\n\nRobsan Dinka\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nIn this blog post, we will complete an implementation of the perceptron algorithm and test it in several experiments.\n\n\n\n\n\nJan 24, 2025\n\n\nRobsan Dinka\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguin_blog/index.html",
    "href": "posts/penguin_blog/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract : In this blog post, we will be classifying penguins based on a list of attributes and measurements using a machine learning model. First, we will make some visuals for the given data to see if we can find an traits common among a species of penguin. Then, we will train our model and test it on data that the model has not seen.\nThe following cell reads in the data set that we will be working with. As you can see in the data frame each row contains both qualitative and quantitative attributes of a single penguin.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nDiscussion : The first table shown displays the number of each species of penguin on each island mentioned in the data. The second table displays the total number of penguins grouped by their species. After looking at these tables we notice that Chinstrap penguins only live on the island Dream and Gentoo penguins only live on the island Biscoe. Now it is easy for us to tell the difference between them because they live in different geographic locations according to our data. Since Adelie penguins live on all three islands, we still need a way to distinguish them from Chinstraps and Gentoo.\n\ntrain.groupby([\"Species\", \"Island\"]).size()\n\nSpecies                                    Island   \nAdelie Penguin (Pygoscelis adeliae)        Biscoe       33\n                                           Dream        45\n                                           Torgersen    42\nChinstrap penguin (Pygoscelis antarctica)  Dream        57\nGentoo penguin (Pygoscelis papua)          Biscoe       98\ndtype: int64\n\n\n\ntrain.groupby(\"Species\").size()\n\nSpecies\nAdelie Penguin (Pygoscelis adeliae)          120\nChinstrap penguin (Pygoscelis antarctica)     57\nGentoo penguin (Pygoscelis papua)             98\ndtype: int64\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns\n\n\n\nDiscussion : In the scatter plots below, penguin body mass to flipper length. On the right side we can see that as a general trend higher body mass is correlated with longer flippers, and a lower body mass is correlated with shorter flippers. The scatter plot on the right identifies the species of each penguin on the graph. Looking at how the top right portion is filled with Gentoo penguins, we can come to the conclusion that Gentoo penguins tend to have a higher body mass and longer flippers.\n\n\nfig, ax  = plt.subplots(1,2, figsize = (14,4))\n\np1 = sns.scatterplot(X_train, x=\"Body Mass (g)\", y= \"Flipper Length (mm)\", ax = ax[0], color= \"blue\")\np2 = sns.scatterplot(train, x =\"Body Mass (g)\", y= \"Flipper Length (mm)\", ax = ax[1], hue = \"Species\", style = \"Species\")\n\n\n\n\n\n\n\n\nDiscussion : In the scatter plot below were Delta 13 C is compared to culmen length, we see that Chinstraps are concentrated at the top right were as the Adelie are more towards the bottom left. This shows that Chinstraps typically have a higher Delta 13 C and longer culmens.\n\nfig , ax = plt.subplots(1, figsize = (4,4))\n\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Delta 13 C (o/oo)\", color = \"red\", hue =\"Species\", style = \"Species\")\n\n\n\n\n\n\n\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\nX_train.columns\n\nIndex(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n       'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)',\n       'Island_Biscoe', 'Island_Dream', 'Island_Torgersen',\n       'Stage_Adult, 1 Egg Stage', 'Clutch Completion_No',\n       'Clutch Completion_Yes', 'Sex_FEMALE', 'Sex_MALE'],\n      dtype='object')\n\n\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nisland = ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nall_qual_cols = [\"Clutch Completion\", \"Sex\", 'Island']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    print(LR.score(X_train[cols], y_train))\n    \n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score.\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.95703125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.9453125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\n0.8359375\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.8203125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\n\n\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.9375\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\n0.80078125\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.9609375\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\n0.94140625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.78515625\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\n\n\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.94921875\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\n0.87109375\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.87109375\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\n0.97265625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.86328125\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\n0.92578125\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\n0.94921875\n\n\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"] got 0.7578125\n# cols = [\"Flipper Length (mm)\", \"Culmen Length (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"] got 0.87109375\n# cols = [\"Body Mass (g)\", \"Culmen Length (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"] got 0.83203125\n# cols = [\"Flipper Length (mm)\", \"Culmen Length (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"] got 0.9453125\n# Top Performers \ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']# got 0.99609375\ncols2 = ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']# got 0.99609375\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.99609375\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\n\nLR2 = LogisticRegression()\nLR2.fit(X_test[cols], y_test)\nLR2.score(X_test[cols], y_test)\n\nc:\\Users\\robsa\\anaconda3\\envs\\ml-451\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n1.0\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR2, X_test[cols], y_test)\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = LR.predict(X_train[cols])\nconfusion_matrix(y_train, y_pred)\n\narray([[108,   0,   0],\n       [  1,  55,   0],\n       [  0,   0,  92]], dtype=int64)\n\n\n\ny_pred2 = LR.predict(X_test[cols])\nconfusion_matrix(y_test, y_pred2)\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]], dtype=int64)\n\n\nAccording to the to the confusion matrix, there is a small probablity that a Chinstrap penguin would be predicted to be an Adelie penguin.\nDiscussion : We learned that the geographic location of penguins important for determining the species of penguin. Culmen length and depth also are big indicators that helped out model predict accurately."
  }
]